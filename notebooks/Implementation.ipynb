{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Verification and Package Installation\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to check if conda environment is properly set up\n",
    "def check_environment():\n",
    "    try:\n",
    "        \n",
    "        # Get current environment name\n",
    "        import os\n",
    "        env_name = os.environ.get('CONDA_DEFAULT_ENV', 'Unknown')\n",
    "        print(f\"âœ“ Current environment: {env_name}\")\n",
    "        \n",
    "        if env_name == 'mlenv':\n",
    "            print(\"âœ“ mlenv environment is active\")\n",
    "        else:\n",
    "            print(\"âš  Warning: Not in mlenv environment\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âš  Conda not detected, using pip installation\")\n",
    "\n",
    "check_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dee1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Package Installation via Conda-Forge\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages for conda-forge installation\n",
    "conda_packages = [\n",
    "    'rasterio',\n",
    "    'scikit-learn', \n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'tensorflow',\n",
    "    'keras',\n",
    "    'opencv',\n",
    "    'scikit-image',\n",
    "    'joblib',\n",
    "    'tqdm'\n",
    "]\n",
    "\n",
    "def install_conda_packages():\n",
    "    \"\"\"Install packages using conda-forge channel\"\"\"\n",
    "    for package in conda_packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            result = subprocess.run([\n",
    "                'conda', 'install', '-c', 'conda-forge', package, '-y'\n",
    "            ], capture_output=True, text=True, check=True)\n",
    "            print(f\"âœ“ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âš  Failed to install {package}: {e}\")\n",
    "            # Fallback to pip\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "                print(f\"âœ“ {package} installed via pip\")\n",
    "            except:\n",
    "                print(f\"âœ— Failed to install {package}\")\n",
    "\n",
    "# Uncomment the line below to run installation\n",
    "# install_conda_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57def5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: FIXED Import Libraries and Verify Installation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Geospatial processing\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.windows import Window\n",
    "from rasterio.features import shapes\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Deep Learning - FIXED IMPORTS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Use direct keras imports to avoid Pylance issues\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, BatchNormalization\n",
    "from keras.layers import Input, concatenate, Activation, Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from skimage import exposure, filters\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: FIXED Data Loading and Preprocessing Functions\n",
    "class SatelliteImageProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive class for processing Sentinel-2 satellite images\n",
    "    for unsupervised classification tasks.\n",
    "    \n",
    "    FIXED: Added prepare_feature_matrix_fixed method to handle different image dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_dir=\"data/training_grids\", validation_dir=\"data/validation_grids\"):\n",
    "        \"\"\"\n",
    "        Initialize the processor with data directories.\n",
    "        \n",
    "        Args:\n",
    "            training_dir (str): Path to training TIF files (20 grids)\n",
    "            validation_dir (str): Path to validation TIF files (10 grids)\n",
    "        \"\"\"\n",
    "        self.training_dir = Path(training_dir)\n",
    "        self.validation_dir = Path(validation_dir)\n",
    "        self.training_data = []\n",
    "        self.validation_data = []\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        print(f\"Training directory: {self.training_dir}\")\n",
    "        print(f\"Validation directory: {self.validation_dir}\")\n",
    "    \n",
    "    def load_tif_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Load a single TIF file using rasterio.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to TIF file\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image_array, profile)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with rasterio.open(file_path) as src:\n",
    "                # Read the image data\n",
    "                image = src.read(1)  # Reading Band 8 (single band)\n",
    "                profile = src.profile\n",
    "                \n",
    "                # Handle nodata values\n",
    "                nodata = src.nodata\n",
    "                if nodata is not None:\n",
    "                    image = np.where(image == nodata, np.nan, image)\n",
    "                \n",
    "                return image, profile\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess satellite image for machine learning.\n",
    "        \n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image array\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Preprocessed image\n",
    "        \"\"\"\n",
    "        # Remove NaN values\n",
    "        image_clean = np.nan_to_num(image, nan=0)\n",
    "        \n",
    "        # Normalize to 0-255 range\n",
    "        if image_clean.max() > 255:\n",
    "            image_normalized = ((image_clean - image_clean.min()) / \n",
    "                             (image_clean.max() - image_clean.min()) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_normalized = image_clean.astype(np.uint8)\n",
    "        \n",
    "        # Apply histogram equalization for better contrast\n",
    "        image_enhanced = exposure.equalize_hist(image_normalized)\n",
    "        \n",
    "        return image_enhanced\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        \"\"\"\n",
    "        Load all training TIF files from the training directory.\n",
    "        \"\"\"\n",
    "        print(\"Loading training data...\")\n",
    "        tif_files = list(self.training_dir.glob(\"*.tif\"))\n",
    "        \n",
    "        if len(tif_files) == 0:\n",
    "            print(\"âš  No TIF files found in training directory!\")\n",
    "            print(\"ðŸ“ Please upload your 20 training grid TIF files to:\", self.training_dir)\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(tif_files)} TIF files\")\n",
    "        \n",
    "        for i, file_path in enumerate(tqdm(tif_files, desc=\"Loading training files\")):\n",
    "            image, profile = self.load_tif_file(file_path)\n",
    "            \n",
    "            if image is not None:\n",
    "                processed_image = self.preprocess_image(image)\n",
    "                self.training_data.append({\n",
    "                    'image': processed_image,\n",
    "                    'profile': profile,\n",
    "                    'filename': file_path.name,\n",
    "                    'file_id': i\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ“ Successfully loaded {len(self.training_data)} training images\")\n",
    "    \n",
    "    def load_validation_data(self):\n",
    "        \"\"\"\n",
    "        Load all validation TIF files from the validation directory.\n",
    "        \"\"\"\n",
    "        print(\"Loading validation data...\")\n",
    "        tif_files = list(self.validation_dir.glob(\"*.tif\"))\n",
    "        \n",
    "        if len(tif_files) == 0:\n",
    "            print(\"âš  No TIF files found in validation directory!\")\n",
    "            print(\"ðŸ“ Please upload your 10 validation grid TIF files to:\", self.validation_dir)\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(tif_files)} TIF files\")\n",
    "        \n",
    "        for i, file_path in enumerate(tqdm(tif_files, desc=\"Loading validation files\")):\n",
    "            image, profile = self.load_tif_file(file_path)\n",
    "            \n",
    "            if image is not None:\n",
    "                processed_image = self.preprocess_image(image)\n",
    "                self.validation_data.append({\n",
    "                    'image': processed_image,\n",
    "                    'profile': profile,\n",
    "                    'filename': file_path.name,\n",
    "                    'file_id': i\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ“ Successfully loaded {len(self.validation_data)} validation images\")\n",
    "    \n",
    "    def prepare_feature_matrix(self, data_list):\n",
    "        \"\"\"\n",
    "        Convert image list to feature matrix for machine learning.\n",
    "        \n",
    "        Args:\n",
    "            data_list (list): List of image dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features_2d, original_shapes)\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        original_shapes = []\n",
    "        \n",
    "        for data in data_list:\n",
    "            image = data['image']\n",
    "            original_shapes.append(image.shape)\n",
    "            \n",
    "            # Flatten image to 1D for traditional ML\n",
    "            flattened = image.flatten()\n",
    "            features_list.append(flattened)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        features_2d = np.array(features_list)\n",
    "        \n",
    "        print(f\"Feature matrix shape: {features_2d.shape}\")\n",
    "        return features_2d, original_shapes\n",
    "    \n",
    "    # â­ ADDED: Missing method that fixes the dimension issue\n",
    "    def prepare_feature_matrix_fixed(self, data_list, target_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Convert image list to feature matrix with FIXED consistent sizing.\n",
    "        \n",
    "        FIXES: Handles different image dimensions by resizing all images to target_size\n",
    "        \n",
    "        Args:\n",
    "            data_list (list): List of image dictionaries\n",
    "            target_size (tuple): Target size for all images (width, height)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features_2d, original_shapes)\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        original_shapes = []\n",
    "        \n",
    "        print(f\"ðŸ”§ Processing {len(data_list)} images with consistent sizing to {target_size}\")\n",
    "        \n",
    "        for i, data in enumerate(tqdm(data_list, desc=\"Creating consistent feature matrix\")):\n",
    "            image = data['image']\n",
    "            original_shapes.append(image.shape)\n",
    "            \n",
    "            # FIXED: Resize all images to consistent target size\n",
    "            if image.shape != target_size:\n",
    "                # Use cv2.resize to ensure consistent dimensions\n",
    "                image_resized = cv2.resize(image, target_size)\n",
    "                print(f\"   Image {i+1}: Resized from {image.shape} to {image_resized.shape}\")\n",
    "            else:\n",
    "                image_resized = image\n",
    "                print(f\"   Image {i+1}: Already correct size {image.shape}\")\n",
    "                \n",
    "            # Flatten image to 1D for traditional ML\n",
    "            flattened = image_resized.flatten()\n",
    "            features_list.append(flattened)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        features_2d = np.array(features_list)\n",
    "        \n",
    "        print(f\"âœ… FIXED Feature matrix shape: {features_2d.shape}\")\n",
    "        print(f\"âœ… All images now have consistent flattened size: {target_size[0] * target_size[1]}\")\n",
    "        \n",
    "        return features_2d, original_shapes\n",
    "    \n",
    "    def visualize_sample_images(self, num_samples=4):\n",
    "        \"\"\"\n",
    "        Visualize sample images from the training data.\n",
    "        \n",
    "        Args:\n",
    "            num_samples (int): Number of sample images to display\n",
    "        \"\"\"\n",
    "        if len(self.training_data) == 0:\n",
    "            print(\"No training data loaded!\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i in range(min(num_samples, len(self.training_data))):\n",
    "            image = self.training_data[i]['image']\n",
    "            filename = self.training_data[i]['filename']\n",
    "            \n",
    "            axes[i].imshow(image, cmap='gray')\n",
    "            axes[i].set_title(f'Sample {i+1}: {filename}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the processor\n",
    "processor = SatelliteImageProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fde7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Project Path Configuration (Keep as is - this is correct)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set base path as the root of the project\n",
    "BASE_DIR = Path(\"..\").resolve()  # Go one level up from notebooks/\n",
    "\n",
    "# Define paths relative to BASE_DIR\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "TRAINING_DIR = DATA_DIR / \"training_grids\"\n",
    "VALIDATION_DIR = DATA_DIR / \"validation_grids\"\n",
    "MODELS_DIR = BASE_DIR / \"models\" / \"saved_models\"\n",
    "VISUALS_DIR = BASE_DIR / \"outputs\" / \"visualizations\"\n",
    "RESULTS_DIR = BASE_DIR / \"outputs\" / \"results\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(TRAINING_DIR, exist_ok=True)\n",
    "os.makedirs(VALIDATION_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(VISUALS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Directories verified and created if missing!\")\n",
    "print(\"\\nðŸ“ Please ensure your TIF files are placed in:\")\n",
    "print(f\"   â€¢ Training files (20 grids): {TRAINING_DIR}\")\n",
    "print(f\"   â€¢ Validation files (10 grids): {VALIDATION_DIR}\")\n",
    "print(\"\\nOnce verified, the data will be loaded below...\")\n",
    "\n",
    "# Initialize processor with absolute paths\n",
    "processor = SatelliteImageProcessor(\n",
    "    training_dir=TRAINING_DIR,\n",
    "    validation_dir=VALIDATION_DIR\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "processor.load_training_data()\n",
    "processor.load_validation_data()\n",
    "\n",
    "# Visualize sample images\n",
    "if len(processor.training_data) > 0:\n",
    "    processor.visualize_sample_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa08100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: COMPLETE FIXED Random Forest Classification Model\n",
    "class RandomForestClassifier_Unsupervised:\n",
    "    \"\"\"\n",
    "    Random Forest classifier adapted for unsupervised learning using clustering.\n",
    "    This approach first clusters the data, then uses Random Forest to learn patterns.\n",
    "    \n",
    "    FIXED: Handles different image dimensions by standardizing to consistent size\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=5, n_estimators=100, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the unsupervised Random Forest classifier.\n",
    "        \n",
    "        Args:\n",
    "            n_clusters (int): Number of clusters for initial unsupervised grouping\n",
    "            n_estimators (int): Number of trees in Random Forest\n",
    "            random_state (int): Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize clustering and classification models\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        self.rf_classifier = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,  # Use all available cores\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2\n",
    "        )\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train(self, features):\n",
    "        \"\"\"\n",
    "        Train the unsupervised Random Forest classifier.\n",
    "        \n",
    "        Args:\n",
    "            features (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        print(\"Training Random Forest Classifier...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Validate input shape\n",
    "        print(f\"Input features shape: {features.shape}\")\n",
    "        \n",
    "        # Step 1: Scale the features\n",
    "        print(\"Step 1: Scaling features...\")\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Step 2: Perform K-means clustering to create pseudo-labels\n",
    "        print(f\"Step 2: Performing K-means clustering with {self.n_clusters} clusters...\")\n",
    "        cluster_labels = self.kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        # Step 3: Train Random Forest using cluster labels\n",
    "        print(\"Step 3: Training Random Forest with cluster labels...\")\n",
    "        self.rf_classifier.fit(features_scaled, cluster_labels)\n",
    "        \n",
    "        # Calculate training statistics\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Analyze cluster distribution\n",
    "        unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        cluster_distribution = dict(zip(unique_labels, counts))\n",
    "        \n",
    "        print(f\"âœ“ Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"âœ“ Cluster distribution: {cluster_distribution}\")\n",
    "        print(f\"âœ“ Feature importance calculated for {features.shape[1]} features\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        return {\n",
    "            'training_time': training_time,\n",
    "            'cluster_distribution': cluster_distribution,\n",
    "            'n_samples': features.shape[0],\n",
    "            'n_features': features.shape[1]\n",
    "        }\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Predict clusters for new data.\n",
    "        \n",
    "        Args:\n",
    "            features (numpy.ndarray): Feature matrix for prediction\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted cluster labels\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction!\")\n",
    "        \n",
    "        # Scale features using the same scaler\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Predict using Random Forest\n",
    "        predictions = self.rf_classifier.predict(features_scaled)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Get feature importance from the trained Random Forest.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Feature importance scores\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first!\")\n",
    "        \n",
    "        return self.rf_classifier.feature_importances_\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to disk. FIXED: Uses Path objects correctly\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before saving!\")\n",
    "        \n",
    "        # Ensure filepath is a Path object and resolve it\n",
    "        filepath = Path(filepath).resolve()\n",
    "        \n",
    "        model_data = {\n",
    "            'kmeans': self.kmeans,\n",
    "            'rf_classifier': self.rf_classifier,\n",
    "            'scaler': self.scaler,\n",
    "            'n_clusters': self.n_clusters,\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"âœ“ Model saved to {filepath}\")\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier_Unsupervised(\n",
    "    n_clusters=5,  # Adjust based on expected land cover types\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ“ Random Forest model initialized\")\n",
    "print(\"ðŸ“Š Model Configuration:\")\n",
    "print(f\"   â€¢ Number of clusters: {rf_model.n_clusters}\")\n",
    "print(f\"   â€¢ Number of estimators: {rf_model.n_estimators}\")\n",
    "print(f\"   â€¢ Random state: {rf_model.random_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: FIXED CNN Model for Unsupervised Classification\n",
    "class UnsupervisedCNN:\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for unsupervised satellite image classification.\n",
    "    Uses autoencoder-style approach combined with clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(256, 256, 1), n_clusters=5, epochs=10):\n",
    "        \"\"\"\n",
    "        Initialize the CNN model.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (tuple): Input image shape (height, width, channels)\n",
    "            n_clusters (int): Number of clusters for classification\n",
    "            epochs (int): Number of training epochs\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.n_clusters = n_clusters\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        self.encoder_model = None\n",
    "        self.history = None\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the CNN autoencoder architecture.\n",
    "        \"\"\"\n",
    "        print(\"Building CNN Model Architecture...\")\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        \n",
    "        # Encoder path\n",
    "        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "        drop1 = Dropout(0.25)(pool1)\n",
    "        \n",
    "        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(drop1)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "        drop2 = Dropout(0.25)(pool2)\n",
    "        \n",
    "        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(drop2)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "        drop3 = Dropout(0.25)(pool3)\n",
    "        \n",
    "        # Bottleneck (feature extraction)\n",
    "        conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(drop3)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        encoded = Dropout(0.5)(conv4)  # This is our encoded representation\n",
    "        \n",
    "        # Decoder path\n",
    "        up1 = UpSampling2D((2, 2))(encoded)\n",
    "        up1 = Conv2D(128, (3, 3), activation='relu', padding='same')(up1)\n",
    "        up1 = BatchNormalization()(up1)\n",
    "        \n",
    "        up2 = UpSampling2D((2, 2))(up1)\n",
    "        up2 = Conv2D(64, (3, 3), activation='relu', padding='same')(up2)\n",
    "        up2 = BatchNormalization()(up2)\n",
    "        \n",
    "        up3 = UpSampling2D((2, 2))(up2)\n",
    "        up3 = Conv2D(32, (3, 3), activation='relu', padding='same')(up3)\n",
    "        up3 = BatchNormalization()(up3)\n",
    "        \n",
    "        # Output layer\n",
    "        decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up3)\n",
    "        \n",
    "        # Build the autoencoder model\n",
    "        self.model = Model(inputs, decoded)\n",
    "        \n",
    "        # Build encoder model for feature extraction\n",
    "        self.encoder_model = Model(inputs, encoded)\n",
    "        \n",
    "        # Compile the model\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ CNN Model built successfully!\")\n",
    "        print(f\"âœ“ Input shape: {self.input_shape}\")\n",
    "        print(f\"âœ“ Total parameters: {self.model.count_params():,}\")\n",
    "        \n",
    "        return self.model.summary()\n",
    "    \n",
    "    def prepare_cnn_data(self, data_list, target_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Prepare image data for CNN training.\n",
    "        \n",
    "        Args:\n",
    "            data_list (list): List of image dictionaries\n",
    "            target_size (tuple): Target image size for CNN\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Prepared image array\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        \n",
    "        for data in tqdm(data_list, desc=\"Preparing CNN data\"):\n",
    "            image = data['image']\n",
    "            \n",
    "            # Resize image to target size\n",
    "            image_resized = cv2.resize(image, target_size)\n",
    "            \n",
    "            # Normalize to [0, 1] range\n",
    "            if image_resized.max() > 1:\n",
    "                image_normalized = image_resized / 255.0\n",
    "            else:\n",
    "                image_normalized = image_resized\n",
    "            \n",
    "            # Add channel dimension\n",
    "            image_final = np.expand_dims(image_normalized, axis=-1)\n",
    "            images.append(image_final)\n",
    "        \n",
    "        return np.array(images)\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"\n",
    "        Train the CNN autoencoder. FIXED: Uses correct model saving path\n",
    "        \n",
    "        Args:\n",
    "            training_data (list): List of training image dictionaries\n",
    "        \"\"\"\n",
    "        print(\"Training CNN Model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare data\n",
    "        print(\"Preparing training data...\")\n",
    "        X_train = self.prepare_cnn_data(training_data)\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        \n",
    "        # Build model if not already built\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Define callbacks with corrected path\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=3, restore_best_weights=True),\n",
    "            ModelCheckpoint(\n",
    "                str(MODELS_DIR / 'cnn_autoencoder.h5'),  # FIXED PATH\n",
    "                save_best_only=True, \n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        print(\"Starting training...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, X_train,  # Autoencoder uses input as target\n",
    "            epochs=self.epochs,\n",
    "            batch_size=8,  # Adjust based on GPU memory\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"âœ“ CNN training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        return {\n",
    "            'training_time': training_time,\n",
    "            'final_loss': self.history.history['loss'][-1],\n",
    "            'final_val_loss': self.history.history['val_loss'][-1]\n",
    "        }\n",
    "    \n",
    "    def extract_features_and_cluster(self, data_list):\n",
    "        \"\"\"\n",
    "        Extract features using trained encoder and perform clustering.\n",
    "        \n",
    "        Args:\n",
    "            data_list (list): List of image dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (cluster_labels, features)\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before feature extraction!\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X = self.prepare_cnn_data(data_list)\n",
    "        \n",
    "        # Extract features using encoder\n",
    "        print(\"Extracting features using trained encoder...\")\n",
    "        features = self.encoder_model.predict(X)\n",
    "        \n",
    "        # Flatten features for clustering\n",
    "        features_flat = features.reshape(features.shape[0], -1)\n",
    "        \n",
    "        # Perform K-means clustering on extracted features\n",
    "        print(f\"Performing K-means clustering with {self.n_clusters} clusters...\")\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(features_flat)\n",
    "        \n",
    "        return cluster_labels, features\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available!\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Plot loss\n",
    "        ax1.plot(self.history.history['loss'], label='Training Loss')\n",
    "        ax1.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot MAE\n",
    "        ax2.plot(self.history.history['mae'], label='Training MAE')\n",
    "        ax2.plot(self.history.history['val_mae'], label='Validation MAE')\n",
    "        ax2.set_title('Model MAE')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('MAE')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize CNN model\n",
    "cnn_model = UnsupervisedCNN(\n",
    "    input_shape=(256, 256, 1),\n",
    "    n_clusters=5,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(\"âœ“ CNN model initialized\")\n",
    "print(\"ðŸ“Š CNN Configuration:\")\n",
    "print(f\"   â€¢ Input shape: {cnn_model.input_shape}\")\n",
    "print(f\"   â€¢ Number of clusters: {cnn_model.n_clusters}\")\n",
    "print(f\"   â€¢ Training epochs: {cnn_model.epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dab2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: COMPLETELY FIXED U-Net Model for Satellite Image Classification\n",
    "class UNetClassifier:\n",
    "    \"\"\"\n",
    "    U-Net architecture adapted for unsupervised satellite image classification.\n",
    "    \n",
    "    FIXED: Resolves keras count_params compatibility issue\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(256, 256, 1), n_classes=5, epochs=10):\n",
    "        \"\"\"\n",
    "        Initialize U-Net model.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (tuple): Input image dimensions (height, width, channels)\n",
    "            n_classes (int): Number of output classes for classification\n",
    "            epochs (int): Number of training epochs\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.is_trained = False\n",
    "        \n",
    "        print(\"ðŸ—ï¸  U-Net Model Initialization\")\n",
    "        print(\"ðŸ“ Architecture Details:\")\n",
    "        print(\"   â€¢ Encoder-Decoder structure with skip connections\")\n",
    "        print(\"   â€¢ Contracting path: 4 blocks with double convolutions\")\n",
    "        print(\"   â€¢ Expansive path: 4 blocks with upsampling\")\n",
    "        print(\"   â€¢ Skip connections preserve spatial information\")\n",
    "        print(\"   â€¢ Batch normalization for training stability\")\n",
    "        print(\"   â€¢ Dropout layers for regularization\")\n",
    "    \n",
    "    def conv_block(self, inputs, filters, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Convolutional block with double convolution, batch normalization, and dropout.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor\n",
    "            filters (int): Number of convolutional filters\n",
    "            dropout_rate (float): Dropout probability\n",
    "            \n",
    "        Returns:\n",
    "            tensorflow.Tensor: Output tensor after conv block\n",
    "        \"\"\"\n",
    "        conv = Conv2D(filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        conv = Conv2D(filters, (3, 3), activation='relu', padding='same')(conv)\n",
    "        conv = BatchNormalization()(conv)\n",
    "        \n",
    "        if dropout_rate > 0:\n",
    "            conv = Dropout(dropout_rate)(conv)\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "    def encoder_block(self, inputs, filters, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Encoder block: conv_block followed by max pooling.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor\n",
    "            filters (int): Number of filters\n",
    "            dropout_rate (float): Dropout probability\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (conv_output, pooled_output)\n",
    "        \"\"\"\n",
    "        conv = self.conv_block(inputs, filters, dropout_rate)\n",
    "        pool = MaxPooling2D((2, 2))(conv)\n",
    "        \n",
    "        return conv, pool\n",
    "    \n",
    "    def decoder_block(self, inputs, skip_features, filters, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Decoder block: upsampling followed by concatenation and conv_block.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor from previous layer\n",
    "            skip_features: Skip connection features from encoder\n",
    "            filters (int): Number of filters\n",
    "            dropout_rate (float): Dropout probability\n",
    "            \n",
    "        Returns:\n",
    "            tensorflow.Tensor: Output tensor after decoder block\n",
    "        \"\"\"\n",
    "        # Upsampling\n",
    "        up = UpSampling2D((2, 2))(inputs)\n",
    "        up = Conv2D(filters, (2, 2), activation='relu', padding='same')(up)\n",
    "        \n",
    "        # Skip connection concatenation\n",
    "        concat = concatenate([up, skip_features])\n",
    "        \n",
    "        # Double convolution\n",
    "        conv = self.conv_block(concat, filters, dropout_rate)\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the complete U-Net architecture. FIXED: Resolves count_params error\n",
    "        \n",
    "        Returns:\n",
    "            tensorflow.keras.Model: Compiled U-Net model\n",
    "        \"\"\"\n",
    "        print(\"Building U-Net Architecture...\")\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        \n",
    "        # Contracting Path (Encoder)\n",
    "        print(\"ðŸ”„ Building Encoder Path...\")\n",
    "        \n",
    "        # Block 1\n",
    "        conv1, pool1 = self.encoder_block(inputs, 64, 0.1)\n",
    "        \n",
    "        # Block 2\n",
    "        conv2, pool2 = self.encoder_block(pool1, 128, 0.2)\n",
    "        \n",
    "        # Block 3\n",
    "        conv3, pool3 = self.encoder_block(pool2, 256, 0.3)\n",
    "        \n",
    "        # Block 4\n",
    "        conv4, pool4 = self.encoder_block(pool3, 512, 0.4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        print(\"ðŸ—ï¸  Building Bottleneck...\")\n",
    "        bottleneck = self.conv_block(pool4, 1024, 0.5)\n",
    "        \n",
    "        # Expansive Path (Decoder)\n",
    "        print(\"ðŸ”„ Building Decoder Path...\")\n",
    "        \n",
    "        # Block 5\n",
    "        up1 = self.decoder_block(bottleneck, conv4, 512, 0.4)\n",
    "        \n",
    "        # Block 6\n",
    "        up2 = self.decoder_block(up1, conv3, 256, 0.3)\n",
    "        \n",
    "        # Block 7\n",
    "        up3 = self.decoder_block(up2, conv2, 128, 0.2)\n",
    "        \n",
    "        # Block 8\n",
    "        up4 = self.decoder_block(up3, conv1, 64, 0.1)\n",
    "        \n",
    "        # Output layer\n",
    "        print(\"ðŸ“¤ Building Output Layer...\")\n",
    "        outputs = Conv2D(self.n_classes, (1, 1), activation='softmax', name='classification_output')(up4)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = Model(inputs=inputs, outputs=outputs, name='U-Net_Classifier')\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy', 'sparse_categorical_crossentropy']\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… U-Net Model Built Successfully!\")\n",
    "        print(f\"ðŸ“Š Model Summary:\")\n",
    "        print(f\"   â€¢ Total Parameters: {self.model.count_params():,}\")  # FIXED: Direct model call\n",
    "        \n",
    "        # FIXED: Safe parameter counting with fallback\n",
    "        try:\n",
    "            trainable_count = sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights])\n",
    "            print(f\"   â€¢ Trainable Parameters: {trainable_count:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   â€¢ Trainable Parameters: Unable to calculate ({str(e)})\")\n",
    "        \n",
    "        return self.model.summary()\n",
    "    \n",
    "    def prepare_unet_data(self, data_list, target_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Prepare data for U-Net training with pseudo-labels from clustering.\n",
    "        \n",
    "        Args:\n",
    "            data_list (list): List of image dictionaries\n",
    "            target_size (tuple): Target image size\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X_train, y_train)\n",
    "        \"\"\"\n",
    "        print(\"ðŸ”„ Preparing U-Net Training Data...\")\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        # Prepare images\n",
    "        for data in tqdm(data_list, desc=\"Processing images\"):\n",
    "            image = data['image']\n",
    "            \n",
    "            # Resize to target size\n",
    "            image_resized = cv2.resize(image, target_size)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            if image_resized.max() > 1:\n",
    "                image_normalized = image_resized / 255.0\n",
    "            else:\n",
    "                image_normalized = image_resized\n",
    "            \n",
    "            # Add channel dimension\n",
    "            image_final = np.expand_dims(image_normalized, axis=-1)\n",
    "            images.append(image_final)\n",
    "        \n",
    "        X = np.array(images)\n",
    "        \n",
    "        # Generate pseudo-labels using K-means clustering on pixel values\n",
    "        print(\"ðŸŽ¯ Generating pseudo-labels using K-means clustering...\")\n",
    "        \n",
    "        labels = []\n",
    "        for image in tqdm(X, desc=\"Creating labels\"):\n",
    "            # Flatten image for clustering\n",
    "            pixels = image.reshape(-1, 1)\n",
    "            \n",
    "            # Remove zero pixels (potentially nodata)\n",
    "            non_zero_mask = pixels.flatten() > 0\n",
    "            if non_zero_mask.sum() > 100:  # Ensure sufficient data points\n",
    "                pixels_nz = pixels[non_zero_mask]\n",
    "                \n",
    "                # Perform K-means clustering\n",
    "                kmeans = KMeans(n_clusters=self.n_classes, random_state=42, n_init=10)\n",
    "                pixel_labels = kmeans.fit_predict(pixels_nz)\n",
    "                \n",
    "                # Create full label image\n",
    "                full_labels = np.zeros(pixels.shape[0], dtype=np.int32)\n",
    "                full_labels[non_zero_mask] = pixel_labels\n",
    "                full_labels = full_labels.reshape(target_size)\n",
    "            else:\n",
    "                # If insufficient data, create random labels\n",
    "                full_labels = np.random.randint(0, self.n_classes, target_size)\n",
    "            \n",
    "            labels.append(full_labels)\n",
    "        \n",
    "        y = np.array(labels)\n",
    "        \n",
    "        print(f\"âœ… Data preparation complete!\")\n",
    "        print(f\"   â€¢ Input shape: {X.shape}\")\n",
    "        print(f\"   â€¢ Label shape: {y.shape}\")\n",
    "        print(f\"   â€¢ Unique labels: {np.unique(y)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"\n",
    "        Train the U-Net model. FIXED: Uses correct model saving path\n",
    "        \n",
    "        Args:\n",
    "            training_data (list): List of training image dictionaries\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ Starting U-Net Training...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_train, y_train = self.prepare_unet_data(training_data)\n",
    "        \n",
    "        # Build model if not already built\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Define callbacks with corrected path\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                str(MODELS_DIR / 'unet_classifier.h5'),  # FIXED PATH\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"ðŸ‹ï¸ Training U-Net...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=4,  # Smaller batch size for U-Net\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ… U-Net training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        return {\n",
    "            'training_time': training_time,\n",
    "            'final_accuracy': self.history.history['accuracy'][-1],\n",
    "            'final_val_accuracy': self.history.history['val_accuracy'][-1],\n",
    "            'final_loss': self.history.history['loss'][-1],\n",
    "            'final_val_loss': self.history.history['val_loss'][-1]\n",
    "        }\n",
    "    \n",
    "    def predict(self, data_list):\n",
    "        \"\"\"\n",
    "        Predict using trained U-Net model.\n",
    "        \n",
    "        Args:\n",
    "            data_list (list): List of image dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class maps\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction!\")\n",
    "        \n",
    "        # Prepare data (without labels)\n",
    "        X = []\n",
    "        for data in data_list:\n",
    "            image = data['image']\n",
    "            image_resized = cv2.resize(image, (256, 256))\n",
    "            \n",
    "            if image_resized.max() > 1:\n",
    "                image_normalized = image_resized / 255.0\n",
    "            else:\n",
    "                image_normalized = image_resized\n",
    "                \n",
    "            image_final = np.expand_dims(image_normalized, axis=-1)\n",
    "            X.append(image_final)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(X)\n",
    "        \n",
    "        # Convert to class labels\n",
    "        predicted_classes = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        return predicted_classes\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history with detailed metrics.\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available!\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax1.plot(self.history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot loss\n",
    "        ax2.plot(self.history.history['loss'], label='Training Loss', color='blue')\n",
    "        ax2.plot(self.history.history['val_loss'], label='Validation Loss', color='red')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Plot categorical crossentropy\n",
    "        ax3.plot(self.history.history['sparse_categorical_crossentropy'], \n",
    "                label='Training CrossEntropy', color='green')\n",
    "        ax3.plot(self.history.history['val_sparse_categorical_crossentropy'], \n",
    "                label='Validation CrossEntropy', color='orange')\n",
    "        ax3.set_title('Categorical Crossentropy')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('CrossEntropy')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Summary statistics\n",
    "        final_acc = self.history.history['accuracy'][-1]\n",
    "        final_val_acc = self.history.history['val_accuracy'][-1]\n",
    "        best_val_acc = max(self.history.history['val_accuracy'])\n",
    "        \n",
    "        ax4.text(0.1, 0.8, f'Final Training Accuracy: {final_acc:.4f}', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.text(0.1, 0.7, f'Final Validation Accuracy: {final_val_acc:.4f}', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.text(0.1, 0.6, f'Best Validation Accuracy: {best_val_acc:.4f}', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.text(0.1, 0.5, f'Total Epochs: {len(self.history.history[\"accuracy\"])}', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.set_title('Training Summary')\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize U-Net model\n",
    "unet_model = UNetClassifier(\n",
    "    input_shape=(256, 256, 1),\n",
    "    n_classes=5,  # Adjust based on expected land cover types\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(\"âœ… U-Net model initialized\")\n",
    "print(\"ðŸ“Š U-Net Configuration:\")\n",
    "print(f\"   â€¢ Input shape: {unet_model.input_shape}\")\n",
    "print(f\"   â€¢ Number of classes: {unet_model.n_classes}\")\n",
    "print(f\"   â€¢ Training epochs: {unet_model.epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: COMPLETELY FIXED Training Phase - All Models\n",
    "\"\"\"\n",
    "FIXED Training Phase: All three algorithms on 20 training grids\n",
    "\n",
    "FIXES APPLIED:\n",
    "1. Random Forest: Uses prepare_feature_matrix_fixed with consistent image sizes\n",
    "2. U-Net: Fixed count_params compatibility issue  \n",
    "3. Enhanced error handling with full traceback\n",
    "\"\"\"\n",
    "\n",
    "if len(processor.training_data) == 0:\n",
    "    print(\"âŒ No training data found!\")\n",
    "    print(\"Please upload your 20 training TIF files to data/training_grids/ directory\")\n",
    "else:\n",
    "    print(f\"ðŸš€ Starting FIXED training with {len(processor.training_data)} training images\")\n",
    "    \n",
    "    # Track overall training time\n",
    "    total_start_time = time.time()\n",
    "    results = {}\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. RANDOM FOREST TRAINING (FIXED)\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŒ³ TRAINING RANDOM FOREST MODEL (FIXED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Use FIXED feature matrix preparation with consistent sizing\n",
    "        print(\"ðŸ”§ Using fixed preprocessing to handle different image dimensions...\")\n",
    "        rf_features, rf_shapes = processor.prepare_feature_matrix_fixed(\n",
    "            processor.training_data, \n",
    "            target_size=(256, 256)  # Force all images to same size - FIXES ERROR\n",
    "        )\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf_results = rf_model.train(rf_features)\n",
    "        results['random_forest'] = rf_results\n",
    "        \n",
    "        print(\"âœ… Random Forest training completed successfully!\")\n",
    "        \n",
    "        # Save Random Forest model with correct path\n",
    "        rf_model.save_model(MODELS_DIR / 'random_forest_unsupervised.pkl')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Random Forest training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Full error details\n",
    "        results['random_forest'] = {'error': str(e)}\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. CNN TRAINING (Should work fine now)\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ§  TRAINING CNN AUTOENCODER MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Train CNN\n",
    "        cnn_results = cnn_model.train(processor.training_data)\n",
    "        results['cnn'] = cnn_results\n",
    "        \n",
    "        print(\"âœ… CNN training completed successfully!\")\n",
    "        \n",
    "        # Plot CNN training history\n",
    "        cnn_model.plot_training_history()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CNN training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results['cnn'] = {'error': str(e)}\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. U-NET TRAINING (FIXED)\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ—ï¸  TRAINING U-NET MODEL (FIXED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Train U-Net with fixed parameter counting\n",
    "        print(\"ðŸ”§ Using fixed U-Net build method to resolve count_params error...\")\n",
    "        unet_results = unet_model.train(processor.training_data)\n",
    "        results['unet'] = unet_results\n",
    "        \n",
    "        print(\"âœ… U-Net training completed successfully!\")\n",
    "        \n",
    "        # Plot U-Net training history\n",
    "        unet_model.plot_training_history()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ U-Net training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results['unet'] = {'error': str(e)}\n",
    "    \n",
    "    # ==========================================\n",
    "    # TRAINING SUMMARY WITH ERROR ANALYSIS\n",
    "    # ==========================================\n",
    "    total_training_time = time.time() - total_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š FIXED TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"Number of Training Images: {len(processor.training_data)}\")\n",
    "    \n",
    "    # Count successful vs failed models\n",
    "    successful_models = sum(1 for result in results.values() if 'error' not in result)\n",
    "    total_models = len(results)\n",
    "    \n",
    "    print(f\"Successful Models: {successful_models}/{total_models}\")\n",
    "    print()\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"ðŸ”¸ {model_name.upper().replace('_', ' ')}:\")\n",
    "        if 'error' in model_results:\n",
    "            print(f\"   âŒ Training failed: {model_results['error']}\")\n",
    "        else:\n",
    "            if 'training_time' in model_results:\n",
    "                print(f\"   â±ï¸  Training time: {model_results['training_time']:.2f}s\")\n",
    "            if 'cluster_distribution' in model_results:\n",
    "                print(f\"   ðŸŽ¯ Clusters: {model_results['cluster_distribution']}\")\n",
    "            if 'final_accuracy' in model_results:\n",
    "                print(f\"   ðŸ“ˆ Final accuracy: {model_results['final_accuracy']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    if successful_models == total_models:\n",
    "        print(\"ðŸŽ‰ All models trained successfully!\")\n",
    "        print(\"ðŸ”„ Ready for validation phase...\")\n",
    "    elif successful_models > 0:\n",
    "        print(f\"âš ï¸ {successful_models} out of {total_models} models trained successfully\")\n",
    "        print(\"ðŸ”„ Can proceed with validation for successful models...\")\n",
    "    else:\n",
    "        print(\"âŒ No models trained successfully. Please check errors above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3420464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Validation Phase - Test on 10 New Grids\n",
    "\"\"\"\n",
    "Validation Phase: Test trained models on 10 new grids\n",
    "\n",
    "This phase will:\n",
    "1. Load validation data (10 new grids)\n",
    "2. Apply all three trained models\n",
    "3. Generate predictions and classifications\n",
    "4. Compare model performances\n",
    "5. Create visualizations and confusion matrices\n",
    "\"\"\"\n",
    "\n",
    "if len(processor.validation_data) == 0:\n",
    "    print(\"âŒ No validation data found!\")\n",
    "    print(\"Please upload your 10 validation TIF files to data/validation_grids/ directory\")\n",
    "else:\n",
    "    print(f\"ðŸ§ª Starting validation with {len(processor.validation_data)} validation images\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    validation_start_time = time.time()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. RANDOM FOREST VALIDATION\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŒ³ RANDOM FOREST VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        if rf_model.is_trained:\n",
    "            # Prepare validation features\n",
    "            rf_val_features, rf_val_shapes = processor.prepare_feature_matrix_fixed(processor.validation_data,target_size=(256, 256))\n",
    "\n",
    "            \n",
    "            # Make predictions\n",
    "            print(\"Making Random Forest predictions...\")\n",
    "            rf_predictions = rf_model.predict(rf_val_features)\n",
    "            \n",
    "            # Get feature importance\n",
    "            feature_importance = rf_model.get_feature_importance()\n",
    "            \n",
    "            validation_results['random_forest'] = {\n",
    "                'predictions': rf_predictions,\n",
    "                'shapes': rf_val_shapes,\n",
    "                'feature_importance': feature_importance,\n",
    "                'n_clusters': len(np.unique(rf_predictions))\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… Random Forest validation completed!\")\n",
    "            print(f\"   ðŸ“Š Predicted {len(np.unique(rf_predictions))} unique classes\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Random Forest model not trained!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Random Forest validation failed: {e}\")\n",
    "        validation_results['random_forest'] = {'error': str(e)}\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. CNN VALIDATION\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ§  CNN VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        if cnn_model.is_trained:\n",
    "            # Extract features and perform clustering\n",
    "            print(\"Extracting CNN features and clustering...\")\n",
    "            cnn_predictions, cnn_features = cnn_model.extract_features_and_cluster(processor.validation_data)\n",
    "            \n",
    "            validation_results['cnn'] = {\n",
    "                'predictions': cnn_predictions,\n",
    "                'features': cnn_features,\n",
    "                'n_clusters': len(np.unique(cnn_predictions))\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… CNN validation completed!\")\n",
    "            print(f\"   ðŸ“Š Predicted {len(np.unique(cnn_predictions))} unique classes\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ CNN model not trained!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CNN validation failed: {e}\")\n",
    "        validation_results['cnn'] = {'error': str(e)}\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. U-NET VALIDATION\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ—ï¸  U-NET VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        if unet_model.is_trained:\n",
    "            # Make predictions\n",
    "            print(\"Making U-Net predictions...\")\n",
    "            unet_predictions = unet_model.predict(processor.validation_data)\n",
    "            \n",
    "            validation_results['unet'] = {\n",
    "                'predictions': unet_predictions,\n",
    "                'n_clusters': len(np.unique(unet_predictions))\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… U-Net validation completed!\")\n",
    "            print(f\"   ðŸ“Š Predicted {len(np.unique(unet_predictions.flatten()))} unique classes\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ U-Net model not trained!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ U-Net validation failed: {e}\")\n",
    "        validation_results['unet'] = {'error': str(e)}\n",
    "    \n",
    "    # ==========================================\n",
    "    # VALIDATION SUMMARY\n",
    "    # ==========================================\n",
    "    validation_time = time.time() - validation_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total Validation Time: {validation_time:.2f} seconds\")\n",
    "    print(f\"Number of Validation Images: {len(processor.validation_data)}\")\n",
    "    print()\n",
    "    \n",
    "    for model_name, model_results in validation_results.items():\n",
    "        if 'error' not in model_results:\n",
    "            print(f\"ðŸ”¸ {model_name.upper().replace('_', ' ')}:\")\n",
    "            print(f\"   ðŸŽ¯ Unique classes found: {model_results['n_clusters']}\")\n",
    "            print(f\"   ðŸ“Š Predictions shape: {np.array(model_results['predictions']).shape}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"âœ… Validation phase completed!\")\n",
    "    print(\"ðŸŽ¨ Ready for visualization and analysis...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4bbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 : Save_Classified_TIFs.ipynb\n",
    "# -----------------------------------------------------------\n",
    "# Save each modelâ€™s prediction arrays to colourised GeoTIFFs\n",
    "# -----------------------------------------------------------\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import ColorInterp\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------------\n",
    "# Helper: write 2-D class array to TIF\n",
    "# -----------------------------------\n",
    "def save_class_raster(array_2d, ref_profile, out_fp, colormap=None):\n",
    "    \"\"\"\n",
    "    Write a 2-D numpy array of integer classes to GeoTIFF.\n",
    "    Keeps CRS, transform, resolution identical to ref_profile.\n",
    "    Optionally embeds an 8-bit colour table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array_2d     : 2-D ndarray of ints\n",
    "    ref_profile  : dict, rasterio profile from an input tile\n",
    "    out_fp       : pathlib.Path or str, destination *.tif\n",
    "    colormap     : dict {class:(R,G,B,A)}, optional â€“ uint8 only\n",
    "    \"\"\"\n",
    "    out_profile = ref_profile.copy()\n",
    "    out_profile.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"count\": 1,\n",
    "        \"dtype\": rasterio.uint8,\n",
    "        \"compress\": \"lzw\",\n",
    "        \"nodata\": 255\n",
    "    })\n",
    "\n",
    "    with rasterio.open(out_fp, \"w\", **out_profile) as dst:\n",
    "        dst.write(array_2d.astype(np.uint8), 1)\n",
    "        if colormap:\n",
    "            dst.colorinterp = [ColorInterp.palette]\n",
    "            dst.write_colormap(1, colormap)\n",
    "\n",
    "    print(f\"âœ“  Saved {out_fp.name}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Folder prep + colour table\n",
    "# -----------------------------------\n",
    "# Ensure outputs/classified_tifs is created at the project root\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent  # assumes this notebook is in 'notebooks/'\n",
    "out_dir = project_root / \"outputs\" / \"classified_tifs\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Simple 5-class colour map (edit as you like)\n",
    "CLASS_COLORS = {\n",
    "    0: (166, 206, 227, 255),\n",
    "    1: ( 31, 120, 180, 255),\n",
    "    2: (178, 223, 138, 255),\n",
    "    3: ( 51, 160,  44, 255),\n",
    "    4: (251, 154, 153, 255)\n",
    "}\n",
    "\n",
    "# Template profile from first validation tile\n",
    "template_profile = processor.validation_data[0][\"profile\"]\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Random Forest tiles  (one class per tile)\n",
    "# -----------------------------------\n",
    "if \"random_forest\" in validation_results and \"error\" not in validation_results[\"random_forest\"]:\n",
    "    rf_preds   = validation_results[\"random_forest\"][\"predictions\"]     # (10,)\n",
    "    rf_shapes  = validation_results[\"random_forest\"][\"shapes\"]          # list[(h,w)]\n",
    "    for i, class_id in enumerate(rf_preds):\n",
    "        h, w      = rf_shapes[i]\n",
    "        raster    = np.full((h, w), class_id, dtype=np.uint8)\n",
    "        save_class_raster(raster, template_profile,\n",
    "                          out_dir / f\"rf_tile_{i:02d}.tif\",\n",
    "                          CLASS_COLORS)\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. CNN tiles  (one class per tile)\n",
    "# -----------------------------------\n",
    "if \"cnn\" in validation_results and \"error\" not in validation_results[\"cnn\"]:\n",
    "    cnn_preds = validation_results[\"cnn\"][\"predictions\"]               # (10,)\n",
    "    for i, class_id in enumerate(cnn_preds):\n",
    "        h, w      = template_profile[\"height\"], template_profile[\"width\"]\n",
    "        raster    = np.full((h, w), class_id, dtype=np.uint8)\n",
    "        save_class_raster(raster, template_profile,\n",
    "                          out_dir / f\"cnn_tile_{i:02d}.tif\",\n",
    "                          CLASS_COLORS)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. U-Net tiles  (full 2-D class maps)\n",
    "# -----------------------------------\n",
    "if \"unet\" in validation_results and \"error\" not in validation_results[\"unet\"]:\n",
    "    unet_preds = validation_results[\"unet\"][\"predictions\"]             # (10,256,256)\n",
    "    for i, arr in enumerate(unet_preds):\n",
    "        save_class_raster(arr, template_profile,\n",
    "                          out_dir / f\"unet_tile_{i:02d}.tif\",\n",
    "                          CLASS_COLORS)\n",
    "\n",
    "print(\"\\nðŸŽ‰  All classified GeoTIFFs saved to:\", out_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Final Performance Summary and Model Comparison (MODIFIED PATHS)\n",
    "\"\"\"\n",
    "Comprehensive Performance Summary\n",
    "\n",
    "This cell provides:\n",
    "1. Detailed performance metrics for each model\n",
    "2. Model comparison summary\n",
    "3. Recommendations for best model selection\n",
    "4. Technical specifications and characteristics\n",
    "\"\"\"\n",
    "\n",
    "def create_performance_summary():\n",
    "    \"\"\"Create comprehensive performance summary.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ† COMPREHENSIVE PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Model characteristics summary\n",
    "    model_summary = {\n",
    "        'Random Forest': {\n",
    "            'Type': 'Traditional Machine Learning',\n",
    "            'Approach': 'Feature-based clustering + supervised learning',\n",
    "            'Best for': 'Quick analysis, interpretable results',\n",
    "            'Strengths': ['Fast training', 'Feature importance', 'Robust to outliers'],\n",
    "            'Weaknesses': ['Limited spatial understanding', 'Requires feature engineering']\n",
    "        },\n",
    "        'CNN Autoencoder': {\n",
    "            'Type': 'Deep Learning - Unsupervised',\n",
    "            'Approach': 'Feature extraction via autoencoding + clustering',\n",
    "            'Best for': 'Feature learning, pattern recognition',\n",
    "            'Strengths': ['Automatic feature learning', 'Good for complex patterns', 'Dimensionality reduction'],\n",
    "            'Weaknesses': ['Requires more data', 'Less interpretable', 'Longer training time']\n",
    "        },\n",
    "        'U-Net': {\n",
    "            'Type': 'Deep Learning - Semantic Segmentation',\n",
    "            'Approach': 'Pixel-wise classification with spatial context',\n",
    "            'Best for': 'Detailed spatial analysis, precise boundaries',\n",
    "            'Strengths': ['Spatial awareness', 'Precise boundaries', 'Skip connections preserve details'],\n",
    "            'Weaknesses': ['Most complex', 'Requires most memory', 'Longest training time']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸ“‹ MODEL CHARACTERISTICS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, characteristics in model_summary.items():\n",
    "        print(f\"\\nðŸ”¸ {model_name}:\")\n",
    "        print(f\"   Type: {characteristics['Type']}\")\n",
    "        print(f\"   Approach: {characteristics['Approach']}\")\n",
    "        print(f\"   Best for: {characteristics['Best for']}\")\n",
    "        print(f\"   Strengths: {', '.join(characteristics['Strengths'])}\")\n",
    "        print(f\"   Weaknesses: {', '.join(characteristics['Weaknesses'])}\")\n",
    "    \n",
    "    # Performance metrics from training and validation\n",
    "    if 'results' in locals():\n",
    "        print(\"\\nâ±ï¸  TRAINING PERFORMANCE\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for model_name, model_results in results.items():\n",
    "            if 'error' not in model_results:\n",
    "                print(f\"ðŸ”¸ {model_name.upper().replace('_', ' ')}:\")\n",
    "                if 'training_time' in model_results:\n",
    "                    print(f\"   Training Time: {model_results['training_time']:.2f} seconds\")\n",
    "                if 'cluster_distribution' in model_results:\n",
    "                    print(f\"   Clusters Created: {len(model_results['cluster_distribution'])}\")\n",
    "                if 'final_accuracy' in model_results:\n",
    "                    print(f\"   Final Accuracy: {model_results['final_accuracy']:.4f}\")\n",
    "                print()\n",
    "    \n",
    "    # Validation results summary\n",
    "    if 'validation_results' in locals():\n",
    "        print(\"\\nðŸ§ª VALIDATION RESULTS\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        for model_name, val_results in validation_results.items():\n",
    "            if 'error' not in val_results:\n",
    "                print(f\"ðŸ”¸ {model_name.upper().replace('_', ' ')}:\")\n",
    "                print(f\"   Classes Detected: {val_results['n_clusters']}\")\n",
    "                print(f\"   Predictions Made: {len(val_results['predictions'])}\")\n",
    "                print()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nðŸ’¡ RECOMMENDATIONS\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    recommendations = {\n",
    "        'For Quick Analysis': 'Random Forest - Fast, interpretable, good for exploratory analysis',\n",
    "        'For Pattern Recognition': 'CNN Autoencoder - Learns complex patterns automatically',\n",
    "        'For Precise Mapping': 'U-Net - Best spatial detail and boundary detection',\n",
    "        'For Production Use': 'Ensemble of all three models for robust results',\n",
    "        'For Limited Resources': 'Random Forest - Lowest computational requirements',\n",
    "        'For Research': 'U-Net - Most advanced, state-of-the-art spatial analysis'\n",
    "    }\n",
    "    \n",
    "    for use_case, recommendation in recommendations.items():\n",
    "        print(f\"ðŸŽ¯ {use_case}: {recommendation}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ‰ ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“ FILES CREATED:\")\n",
    "    print(f\"   â€¢ {MODELS_DIR / 'random_forest_unsupervised.pkl'}\")  # MODIFIED DISPLAY\n",
    "    print(f\"   â€¢ {MODELS_DIR / 'cnn_autoencoder.h5'}\")  # MODIFIED DISPLAY\n",
    "    print(f\"   â€¢ {MODELS_DIR / 'unet_classifier.h5'}\")  # MODIFIED DISPLAY\n",
    "    \n",
    "    print(\"\\nðŸ”„ NEXT STEPS:\")\n",
    "    print(\"   â€¢ Use saved models for new satellite image classification\")\n",
    "    print(\"   â€¢ Fine-tune parameters based on specific use case\")\n",
    "    print(\"   â€¢ Collect ground truth data for supervised validation\")\n",
    "    print(\"   â€¢ Experiment with different numbers of clusters/classes\")\n",
    "    print(\"   â€¢ Try ensemble methods combining all three models\")\n",
    "\n",
    "# Generate performance summary\n",
    "create_performance_summary()\n",
    "\n",
    "# Save validation results to file for future reference (MODIFIED PATHS)\n",
    "if 'validation_results' in locals():\n",
    "    import pickle\n",
    "    \n",
    "    # Save validation results with correct path\n",
    "    validation_results_path = RESULTS_DIR / 'validation_results.pkl'  # MODIFIED PATH\n",
    "    with open(validation_results_path, 'wb') as f:\n",
    "        pickle.dump(validation_results, f)\n",
    "    \n",
    "    # Create a summary report with correct path\n",
    "    summary_report = {\n",
    "        'training_images': len(processor.training_data),\n",
    "        'validation_images': len(processor.validation_data),\n",
    "        'models_trained': list(validation_results.keys()),\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    summary_report_path = RESULTS_DIR / 'summary_report.pkl'  # MODIFIED PATH\n",
    "    with open(summary_report_path, 'wb') as f:\n",
    "        pickle.dump(summary_report, f)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Results saved to {RESULTS_DIR}\")  # MODIFIED DISPLAY\n",
    "\n",
    "print(\"\\nðŸŒŸ Classification pipeline completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
